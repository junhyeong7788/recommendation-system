# 🚀 학습 키워드

## 정의

- SVD (Singular Value Decomposition) : 특이값 분해
  - 임의의 $m X n$ 차원의 행렬 $A$에 대하여 다음과 같이 행렬 분해를 할 수 있다는 `행렬 분해(Decomposition)`의 한 방법
  - $A = U \Sigma V^T$
    - $A$ : $m X n$ 행렬 (rectangular matrix)
    - $U$ : $m X m$ 직교행렬 (orthogonal matrix)
    - $\Sigma$ : $m X n$ 대각행렬 (diagonal matrix)
    - $V$ : $n X n$ 직교행렬 (orthogonal matrix)

## 키워드

- 특이값 분해가 말하는 것 : 직교하는 벡터 집합에 대하여, 선형 변환 후에 그 크기는 변하지만 여전히 직교할 수 있게 되는 그 직교 집합은 무엇인가? 그리고 선형 변환 후의 결과는 무엇인가?

---

# 📝새로 배운 개념

## SVD의 의미

- 직교하는 벡터 집합에 대하여, 선형 변환 후에 그 크기는 변하지만 여전히 직교할 수 있게 되는 그 직교 집합은 여러 개
  - 선형 변환 후의 크기를 singular value라고 하고 크기가 큰 값부터 $\Sigma_1, \Sigma_2, \cdots, \Sigma_n$으로 정렬
  - $A = U \Sigma V^T$
    - $$V = \begin{pmatrix} \vec{x} & \vec{y} \end{pmatrix}, \quad U = \begin{pmatrix} \vec{u}_1 & \vec{u}_2 \end{pmatrix}, \quad \Sigma = \begin{pmatrix} \sigma_1 & 0 \\ 0 & \sigma_2 \end{pmatrix}$$
    - 선형 변환의 관점에서 네 개의 행렬 ($A, U, \Sigma, V$)의 관계를 생각하면 다음과 같다.
    - $AV = U \Sigma$ : 즉, "V에 있는 열벡터 ($\vec{x}$ 혹은 $\vec{y}$)를 행렬A를 통해 선형 변환할 때, 그 크기는 $\Sigma_1, \Sigma_2$만큼 변하지만, 여전히 직교하는 벡터들 $\vec{u}_1, \vec{u}_2$를 찾을 수 있는가? 라고 묻는 것이다.
    - 그러면 V는 orthogonal matrix이므로 $V^-1 = V^T$이기 때문에,
    - $$AV = U \Sigma \quad \rightarrow AVV^T = U \Sigma V^T \rightarrow \quad A = U \Sigma V^T$$
    - 라는 관계가 성립

## SVD의 목적

- $A = U \Sigma V^T = \sigma_1 \vec{u}_1 \vec{v}_1^T + \sigma_2 \vec{u}_2 \vec{v}_2^T + \cdots + \sigma_m \vec{u}_m \vec{v}_m^T$
- 따라서, $\sigma_1 \vec{u}_1 \vec{v}_1^T$ 라는 부분만을 놓고 보면, 이 행렬의 크기는 $\sigma_1$ 의 값에 의해 정해지게 된다.
  - 즉, 우리는 SVD라는 방법을 이용해 A라는 임의의 행렬을 여러개의 A행렬과 동일한 크기를 갖는 여러 개의 행렬로 분해해서 생각할 수 있는데, 분해된 각 행렬의 원소의 값의 크기는 \sigma의 값의 크기에 의해 결정된다.
  - `SVD를 이용해 임의의 행렬 A를 정보량에 따라 여러 layer로 쪼개서 생각할 수 있게 해준다`

---

# ✨ 특이값 분해의 활용

- 특이값 분해는 분해되는 과정보다는 분해된 행렬을 다시 조합하는 과정에서 응용력이 빛을 발함
  - 기존의 $U , \Sigma, V^T$로 분해되어 있던 $A$ 행렬을 특이값 p개만을 이용해 $A'$라는 행렬로 `부분 복원` 할 수 있다.
  - 특이값의 크기에 따라 A의 정보량이 결정되기 때문에 값이 큰 몇 개의 특이값들을 가지고도 충분히 유용한 정보를 유지할 수 있다.
  - 예시1 : `추천 시스템`에서는 사용자와 아이템 간의 평점 행렬을 특이값 분해를 통해 분해하고, 특이값 중 상위 몇 개만을 이용해 사용자와 아이템 간의 평점을 예측하는데 활용한다.
  - 예시2 : 사진, 최대한 중요한 정보들만 부분 복원해서 사용하면 사진의 용량은 줄어들지만 여전히 사진이 보여주고자 하는 내용은 살릴 수 있다.

## 추천시스템에서의 SVD

1. 특이값 분해 과정

- 사용자-아이템 평점 행렬 $R$ (사용자가 영화에 매긴 평점)을 이요해 분해
- $R = U \Sigma V^T$
  - $R$ : 사용자-아이템 평점 행렬
  - $U$ : 사용자-특성(사용자 잠재 요인 : Latent Factors) 행렬
  - $\Sigma$ : 특이값 행렬
  - $V^T$ : 아이템-특성(아이템 잠재 요인) 행렬
  - 이 분해를 통해 사용자와 아이템의 잠재공간을 찾을 수 있으며, 이는 사용자와 아이템 간의 관계를 보다 간결하게 표현

2. 상위 특이값만 사용

- $\Sigma$ 행렬의 특이값 중 상위 k개의 값만 남기고, 나머지는 제거하여 차원 축소
  - 상위 k개의 특이값은 평점 행렬의 가장 중요한 정보(잠재 구조)를 보존
  - 작은 k값을 선택하면 노이즈와 불필요한 데이터를 제거 가능
  - $R \approx U_K \Sigma_K V_K^T$ : 상위 k개의 특이값과 이에 대응하는 행/열만 포함한 행렬

3. 평점 예측

- $R_k$를 다시 계산 -> 사용자와 아이템 간의 새로운 평점(결측값 포함)을 예측
  - 기존에 사용자가 평가하지 않은 아이템의 평점을 예측 가능
  - 예측된 평점이 높은 아이템을 추천리스트로 제공 가능

4. 한계

- Cold Start 문제
- 계산 비용 : 대규모 평점 행렬, 행렬 분해 계산 비용 증가
- 비선형 관계 반영 부족 : SVD는 선형적인 모델, 비선형 관계 반영 어려움

---

# 🔗레퍼런스

## 참고 강의/글

- [특이값 분해(SVD) - 공돌이의 수학정리 노트](https://angeloyeo.github.io/2019/08/01/SVD.html)
