# 🚀 학습 키워드

## 정의

- Factorization Machines : 특성 간의 상호작용을 효율적으로 모델링하는 머신러닝 모델
  - 특히 희소 데이터나 범주형 데이터에서 효과적으로 작동

## 키워드

- 특성 간의 2차 상호작용을 모델링하는 데 최적화된 모델
- 행렬 분해(Matrix Factorization)와 선형 모델의 장점을 결합

---

# 📝새로 배운 개념

## 기본 개념

- 선형회귀와 행렬분해의 장점을 결합한 모델
- FM은 데이터에서 특성 간의 2차 상호작용(특성과 특성의 조합)을 학습하면서도, 모든 특성 쌍을 명시적으로 계산하지 않고 잠재 요인을 통해 이를 효율적으로 표현

  - $\hat{y} = w_0 + \sum_{i=1}^n w_i x_i + \sum_{i=1}^n \sum_{j=i+1}^n \langle v_i, v_j \rangle x_i x_j$
    - $w_0$ : 전역 편향 (모든 데이터에 동일하게 적용되는 상수)
    - $w_i$ : i번째 특성의 가중치
    - $v_i$ : i번째 특성의 잠재 요인 벡터
    - $\langle v_i, v_j \rangle$ : 잠재요인 i번째 특성과 j번째 특성의 상호작용을 나타내는 벡터 내적 (특성 간의 상호작용 모델링)

## FM의 작동 방식

1. 특성 간의 2차 상호작용

- FM은 특성 간의 상호작용을 내적(dot product)으로 모델링
  - $\langle v_i, v_j \rangle = \sum_{f=1}^k v_{i,f} \cdot v_{j,f}$
    - $v_{i,f}$ : i번째 특성의 f번째 잠재 요인
    - $v_{j,f}$ : j번째 특성의 f번째 잠재 요인
    - $k$ : 잠재 요인의 차원 수

2. 잠재 벡터의 활용

- 각 특성은 잠재벡터로 표현, $v_i = [v_{i,1}, v_{i,2}, \dots, v_{i,k}]$
  - $v_i$와 $v_j$의 내적은 특성 i와 j의 상호작용 강도를 나타냄

3. 계산의 효율성

- FM은 모든 특성 쌍을 직접 계산하지 않고, 잠재 벡터의 내적을 사용해 효율적으로 상호작용을 모델링함
- 계산 복잡도는 특성 수에 비례하여 선형적으로 증가

---

# ✨

## FM이 해결하는 문제

1. 특성 간의 상호작용 학습

- 일반적인 선형 모델(Linear Regression)은 개별 특성의 중요도만 학습,
  - $\hat{y} = w_0 + \sum_{i=1}^n w_i x_i$, 특성간의 상호작용($x_1 * x_2$)은 고려하지 않는다.
  - FM은 ($x_1 * x_2$) 와 같은 특성 간의 상호작용을 자동으로 학습

2. 희소 데이터 문제

- 추천 시스템의 사용자-아이템 평점 데이터는 대개 많은 값이 비어 있으므로 희소 데이터이다.
- FM은 모든 특성 쌍의 상호작용을 직접 계산하지 않고, 잠재 벡터로 표현하여 학습 효율성을 높임

## FM의 한계

1. 2차 상호작용까지만 모델링
2. FM이 학습한 잠재 벡터는 직관적으로 해석하기 어렵다
   - 이는 행렬분해기반 모델들의 공통적인 한계
3. 대규모 데이터에서 학습 비용
   - 데이터가 매우 크거나 잠재 요인의 차원이 클 경우, 학습 속도가 느려질 수 있다.

---

# 🔗레퍼런스

## 참고 강의/글

- [Factorization Machines (FM) 설명 및 Tensorflow 구현](https://greeksharifa.github.io/machine_learning/2019/12/21/FM/)
